\documentclass[12pt,UTF8,fntef]{article}
\usepackage[left=1.8cm, right=1.8cm, top=1.8cm, bottom=1.8cm]{geometry}
%\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage[unicode, pdfborder={0 0 0}, bookmarksdepth=-1]{hyperref}
\usepackage[usenames, dvipsnames]{color}
\usepackage[shortlabels, inline]{enumitem}
\usepackage{fancyhdr}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{subfigure}
\usepackage{float}
\usepackage{spreadtab}
%\usepackage{cite}
\usepackage{siunitx}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage{pgfplotstable}

\pgfplotsset{width=15cm}
\pgfplotsset{height=8cm}
\pgfplotsset{compat=1.16}
%\setcitestyle{numbers}

\hypersetup{
    pdftoolbar=true,        % show Acrobat’s toolbar?
    pdfmenubar=true,        % show Acrobat’s menu?
    pdffitwindow=false,     % window fit to page when opened
    pdfstartview={FitH},    % fits the width of the page to the window
    %pdftitle={前瞻語音計畫申請},    % title
    pdfauthor={Jui-Yang Hsu},     % author
    pdfnewwindow=true,      % links in new PDF window
    colorlinks=true,       % false: boxed links; true: colored links
    linkcolor=purple,          % color of internal links (change box color with linkbordercolor)
    citecolor=blue,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=cyan           % color of external links
}

\usepackage[square,numbers]{natbib}
%\bibliographystyle{abbrvnat}
\renewcommand{\baselinestretch}{1.25}
\usepackage{xeCJK}
\usepackage{fontspec}
%\setCJKmainfont{DFFN_R3.TTC}
\setCJKmainfont[BoldFont=Noto Serif CJK TC Bold]{Noto Serif CJK TC}
\title{研究計畫書}
\author{電機工程研究所~~~計算機科學組\\R07921053~~~徐瑞陽 \\ 技術部落格: \texttt{https://sunprinces.github.io/learning/}}
%\email{r07921053@ntu.edu.tw}
\date{}

\begin{document}

\maketitle

近年來深度學習技術已經應用於許多語音的相關領域，並取得突破性的成果，如聲音轉換、智慧問答系統、語音辨識...等，然而這些應用僅限於有豐富標註資料的主流語言。以語音辨識(Automatic Speech Recognition, ASR)為例，利用神經網路與隱式馬可夫模型的混合模型 (DNN-HMM Hybrid Model) 建構以音素 (phoneme) 為單位的聲學模型，再利用發音字典 (lexicon) 與遞歸神經網路 (Recurrent Neural Network) 所訓練的語言模型來做語音辨識為目前最常被使用的架構。然而，這樣的架構除了要有每段語音 (utterance) 對應的翻譯之外，亦需事先用字典及維特比演算法 (Viterbi Algorithm) 對音素及幀 (frame) 做初步對齊後才能進行訓練，因為標註字典的成本昂貴且門檻較高，這樣的設定對於多數非主流研究語言是不切實際的。

因此，Graves 等人提出了端對端語音辨識系統之構想\cite{graves2014towards}，近年來也有許多相關模型被提出，如百度的 DeepSpeech \cite{hannun2014deep}、Google 的 LAS \cite{chan2016listen} (架構如 Figure \ref{fig:monoASR} 所示)...等。這些模型主要以序列到序列模型 (Sequence-to-Sequence Model) 作為核心，配合利用專注機制 (Attention) 的解碼器與以 Connectionist Temporal Classification (CTC) 為目標函數的解碼器，訓練出輸入為音訊，輸出為字位 (grapheme) 的端對端模型，同時整合了聲學模型與語言模型的訓練，並移除需要字典的前提。

然而雖然對標註的要求有所降低，相關文獻中均提到了端對端模型相較 DNN-HMM ，若要達到相同詞錯誤率 (Word Error Rate, WER)的情況下，需要更多訓練資料，對於語料量本來就不足的非主流語言來說，這樣的前提也使得該架構難以應用於實務中。也因此，如何利用多語料語言 (high-resource language) 去輔助訓練少語料語言(low-resource language) 的方法 - 語言自適應訓練 (Language Adaptative Training, LAT) 為近年來語音界的主要研究方向之一。接下來，我首先會介紹我所選擇的方法 - 元學習 (Meta Learning)，再介紹目前 LAT ASR 的主流方法 - 遷移學習 (Transfer Learning) ，及元學習可能會帶來什麼好處，並展示初步結果及待解決的問題；接著介紹其在生成系列的語音任務如語音合成 (Text-to-Speech, TTS)、聲音轉換 (Voice Conversion, VC)的可能應用，最後探討元學習應用在語音領域有何不同之處及使用限制，最後簡述未來可能的發展方向。


\newpage

  \begin{section}{元學習介紹}
~~~~目前在監督式學習 (Supervised Learning) - 給定特定任務的大型資料集，從頭開始訓練模型的情境下，深度學習已經取得了很大的成功。然而，相較於人類在學習新任務時，給定少量樣本便能完成學習的情況，其對資料的利用顯然是沒有效率的。而人類之所以能快速學習，主因是我們有元學習 (Meta Learning)，又稱為學習如何學習 (Learning to Learn) 的能力，能從先前的任務學習過程中，發現可借鑒的通用經驗，利用已習得的技能，達到快速學習新技能的目的。以下會定義元學習，並介紹及分類近年來提出的方法。

    \begin{subsection}{定義}
      ~~~~以下定義 $\theta$ 為想優化的模型參數，$\mathcal{D}$ 為資料集 ($\mathcal{D}^{\text{tr}}$為訓練資料、$\mathcal{D}^{\text{test}}$為驗證資料)，$M$ 為模型 (可以是最近鄰居、類神經網路 $\cdots$)，$\mathcal{A}_M(\mathcal{D})$ 為衡量參數在資料集 $\mathcal{D}$ 上好度的函數， 我們想找到一個模型 $f_\theta: \mathcal{D} \rightarrow M$ 滿足以下元學習的目標。
      \begin{equation}
        \theta^{\star} = \arg \max_\theta \mathbb{E}_{\mathcal{D} \sim p(\mathcal{D}) }[\mathcal{A}_{f_\theta(\mathcal{D^{\text{tr}}})}(\mathcal{D^{\text{test}}})]
      \end{equation}

而不同 $f_\theta$ 的實現方式，也對應了不同的元學習算法，以下介紹目前主流的三種方法。



      %為了更好的理解元學習，我們來看一個其最常被使用的情境 - 少量樣本分類 (Few-shot Classification)。在這樣的情境下，對每個任務 (task) 我們都只有少量的訓練資料，但有多個任務可以學習。舉例來說，我們擁有數個二元分類的資料集 (狗 vs 貓, 火車 vs 飛機, 蘋果 vs 香蕉...)，但每個資料集的每個類別都只有 $N$ 個訓練樣本 ($N \geq 1$)，給定一個新資料集 (鳥 vs 船，且此類別都沒有出現在先前的資料集中)，我們希望訓練出的模型能做到僅利用 $2N$ 張樣本便自適應 (adapt) 到新資料集上。

      %在這樣的情境下，對於每個特定任務我們都只有少量的訓練資料，但有多個任務可以學習，目標為

      %\begin{equation}
   %\theta^\star = \arg \max_\theta \mathbb{E}_{L \subset \mathcal{L}} \, [ \mathbb{E}_{S^L \subset \mathcal{D}, B^L \subset \mathcal{D}}[\sum_{(x,y) \in B^L} \log P_\theta(y|x,S^L)] \,  ]
       %\end{equation}
    \end{subsection}

    \begin{subsection}{基於度量的元學習方法 (Metric-based Approach)}
      $f_\theta$的主體在學習一個特徵編碼器 (feature encoder)，而經過該編碼器編碼後的特徵，再用最近鄰居 (Nearest Neighbor) 算法解決分類問題。如原型網路 (Prototypical Network) \cite{snell2017prototypical}、配對網路 (Matching Network)\cite{vinyals2016matching}...等, 此方法目前僅用於分類。
    \end{subsection}

    \begin{subsection}{基於模型設計的元學習方法 (Model-based Approach)}
      $f_\theta$的主體為一個複雜的模型 (神經網路及其變形)，藉由設計該模型架構，使得輸入資料集後能吐出 $M$的參數，或再接著輸入測試樣本的資料點後直接輸出預測。舉例來說記憶增強網路 (Memory Augmented Network) \cite{santoro2016meta}利用神經圖靈機，建構一個可讀寫的外部記憶，用來儲存訓練樣本以便預測時查找。因其讀寫外部記憶的複雜度為 $\mathcal{O}(|\mathcal{D}|)$，目前僅在少量樣本分類中使用。
    \end{subsection}

    \begin{subsection}{基於梯度優化的元學習方法 (Gradient-based Approach)} \label{opt-meta}
      $f_\theta$的主體為起始參數，之後利用訓練資料做幾步的梯度下降 (Gradient Descent) 後用來在測試資料的預測 \cite{finn2017model, nichol2018first}，也可看成是一種預訓練 (Pretraining) 的方式。需注意的是，利用這組參數直接在新資料集上進行預測並不能得到理想的結果，而是需要先進行幾步的梯度下降更新。
    \end{subsection}
  \end{section}
  \newpage

  \begin{section}{遷移學習應用於 LAT} \label{trans-asr}

  \begin{figure}[ht]
      \centering
      \includegraphics[width=0.6\linewidth]{monoASR.png}
      \caption{Basic learner structure: LAS}
      \label{fig:monoASR}
  \end{figure}

  \vspace{-0.25in}

  \begin{figure}[ht]
    \centering
    \subfigure[Universal ASR: Union the output classes]{%
      \label{fig:universalASR}%
    \includegraphics[width=0.55\linewidth]{UniversalASR.png}}%
    \subfigure[MultiTask ASR: Language-specific decoder]{%
      \label{fig:multitaskASR}%
    \includegraphics[width=0.41\linewidth]{MultiTaskASR.png}}%
    \caption{Baseline model: Universal ASR \& MultiTask ASR}
  \end{figure}

  目前端對端語音辨識模型 LAT 的主流方法為遷移學習。其中 Figure \ref{fig:monoASR} 為基本的模型架構，可以訓練單一語言的模型。Figure \ref{fig:universalASR} 則延伸原先的架構，統一所有訓練語料的輸出類別，可藉由聯集訓練語言的字位或使用語言通用的國際音標來實現；Figure \ref{fig:multitaskASR} 架構則是對不同語言建構各自的解碼器，分別輸出字位，但前面的編碼器是互相共享的。在遷移到目標語言時，我們將輸出層換成適合該語言的層，並重新訓練輸出層，或對整個模型進行微調 (fine-tuning) 以實現 LAT\footnote{此處不考慮將目標語言加入訓練，雖然這通常會讓結果更好，但我們希望訓練的模型對目標語言沒有任何假設}。

~~在編碼器端，為了消弭不同語言的差異，擷取出語言獨立 (language-independent) 的特徵供解碼器使用，常見的作法有加上語言分類器進行對抗訓練 (Adversarial Training)，或加入音素、子字單位 (subword unit)的 CTC 解碼器。
  \end{section}

  \begin{section}{元學習應用於 LAT}
    \begin{subsection}{為何元學習於語音有研究價值}

  \begin{figure}[ht]
      \centering
      \includegraphics[width=0.8\linewidth]{Meta-motivation.png}
      \caption{不同學習之間的比較，實線為初始參數的學習過程，虛線為微調的過程}
      \label{fig:motivation}
  \end{figure}

  遷移學習以一個在多語料(多語言)訓練好的模型為基準，用於學習新的目標語言，如Figure \ref{fig:motivation} 的 (a)、(b)\footnote{圖片來自 \cite{gu2018meta}}，然而問題有二︰
      \begin{enumerate}[itemsep=-1mm]
        \item 需假設訓練語料彼此的最優參數不能相差太多，因在訓練過程中必須同時滿足各語言的目標函數，這相當於對模型施加了正則化 (regularization)，使其無法很好處理語種差異過大，導致正則化強度太大，無法達到最佳表現的情況。
        \item 需假設訓練語料與測試語料的最優參數不能相差太多，否則在損失平面 (loss surface) 上過長的移動，會降低找到好解的可能。 
      \end{enumerate}

      元學習中學習初始參數的方法(\ref{opt-meta}) 很好地解決了上述問題，透由在梯度更新時，將學習過程隱性編碼進所找到的參數中，使得該參數可以快速泛化到不同語言，如 Figure \ref{fig:motivation} 的 (c)所示。
    \end{subsection}
    \begin{subsection}{使用限制及挑戰} \label{meta-problem}
      目前元學習主要應用於機器視覺的少樣本分類、與強化學習，直至去年，才有了第一篇應用於機器翻譯的研究 \cite{gu2018meta}，MetaNMT，訓練多語言翻譯至英文的模型，原因為以下兩點︰
      \begin{enumerate}[itemsep=-1mm]
        \item 要訓練一個成功的機器翻譯或語音辨識系統，難度相當高，不可能僅僅使用數個樣本便達到，因此多數針對少樣本分類問題的元學習演算法不適用，僅有基於優化的演算法 (\ref{opt-meta}) 在以一階梯度近似的前提下才能應用於該情境 (如 FOMAML \cite{finn2017model}、Reptile\cite{nichol2018first})。
        \item 多數學習初始參數的元學習方法雖名為與模型無關 (Model-Agnostic) ，但在使用時仍要求模型架構必須相同 (如網路中每層的神經元數量)，MetaNMT 的處理方式為先將所有語言映射到相同空間，使得輸入層一致進而達到全模型一致；但在語音辨識中，雖然不同語言的輸入同為相同維度的語音特徵，輸出卻是每個語言都不同，因此無法直接套用元學習。
      \end{enumerate}
    \end{subsection}
    \begin{subsection}{初步實驗 - 音素辨識 (Framewise Phoneme Classification)}
      在將元學習應用於端對端語音辨識之前，我先嘗試音素辨識這個較為簡單的語音任務，來驗證其可行性，如 \ref{meta-problem} 所提到的模型架構問題，我放寬了對架構的假設，在每個任務的學習中，允許輸出層以同樣的方法進行權重初始化後再開始訓練，在測試時亦可置換輸出層。除此之外，相比於少樣本分類問題，我將原先對單位任務訓練資料量的限制從少樣本 (Few-shot) 放寬到少語料 (Low-resource)，將對收斂速度的要求從幾步梯度就收斂放寬至快速自適應 (rapid adaptation)，如一個 epoch (約數百步梯度)，以更符合各語音應用的使用情境。

      我選用 TUNDRA \cite{stan2013tundra} 這個語料庫做為評斷標準，每個語言有 $4 \sim 10$小時不等的語料，比較了 MultiTask ASR 與 Meta ASR 兩種模型預訓練於 6 種語言，與完全沒有預訓練 (no-pretrain) 的模型，在目標的兩種語言訓練 5 個 epoch 後在測試資料進行預測的表現。

    \begin{figure}[H]
    \centering
    \hspace{-5.2cm}
    \begin{tikzpicture}[trim axis left, trim axis right]

    \begin{axis}[
      width=0.6\linewidth,
      legend entries={Meta ASR (on \SI{10}{\percent} data), MultiTask ASR (on \SI{10}{\percent} data), no-pretrain (on \SI{10}{\percent} data),no-pretrain (on \SI{20}{\percent} data), no-pretrain (on \SI{50}{\percent} data)} ,
          xlabel = {Number of pretraining steps},
          xmin=0,
          xmax=210,
          grid=both,
          %legend style={at={(0.65,0.62)},anchor=south west},
          legend pos=outer north east,
          ylabel={Phoneme Classification Accuracy}]
    \addplot+[smooth]table{reptile_metaval};
    \addplot+[smooth]table{ft_metaval};
     \addplot[style=ultra thick,dashed,] coordinates {(0,0.557) (200,0.557)};
     \addplot[style=ultra thick,dashed, gray] coordinates {(0,0.589) (200,0.589)};
     \addplot[style=ultra thick,dashed, brown] coordinates {(0,0.628) (200,0.628)};
    \end{axis}
    \end{tikzpicture}
    %\caption{Pretrain on EN, FI, FR, NL, RM, RU, and evaluate on }
    \caption{TUNDRA phoneme classification experiment}
    \label{fig:tundra-exp}
  \end{figure}
  從 Figure \ref{fig:tundra-exp} 可以看出，有預訓練的模型可以用較少的目標語言訓練資料便有較好的表現；且元學習的方法 (Meta ASR) 相較 MultiTask ASR\footnote{這裡並沒有比較 Universal ASR，因預訓練語言以不同的音素集 (Arpabet、GlobalPhone、Prosodylab、IPA)標註，在訓練語料不夠多的情況下，表現不甚理想}，在資料量稀缺的情況下不會過度擬合 (over-fitting) 於預訓練語言，在目標語言的測試資料有較好的表現。
    \end{subsection}
  \end{section}

\begin{section}{未來發展}
  \begin{section}{目前進度} \label{progress}
    在端對端的語音辨識中，我選用 IARPA-BABEL 這個語料庫做為評斷標準，該語料庫包含了 $10$ 餘種語言，每個語言約有 $40 \sim 80$ 小時不等的語料，我已完成訓練在單一語言上的模型，其結果如 Table \ref{table:monoASR}，當中 FLP (Full Language Pack) 為利用該語言所有的訓練語料所得到的結果，而 LLP (Limited Language Pack) 則是根據 Kaldi 處方所選出的約 $10$ 小時的語料訓練而得。除了 MetaASR 的實現，亦會實現 \ref{trans-asr} 提到的 Universal ASR, MultiTask ASR 作為底線 模型 (baseline)。

\begin{table}[ht]
  \centering
\begin{tabular}{l|c|c|l|c|c|}
\cline{2-3} \cline{5-6}
\multicolumn{1}{c|}{}            & CER (LLP) & CER (FLP) & \multicolumn{1}{c|}{} & CER (LLP) & CER (FLP) \\ \hline
\multicolumn{1}{|l|}{Assamese}   & 83        & 46.4      & Tamil                 & 100       & 52.3      \\ \hline
\multicolumn{1}{|l|}{Bengali}    & 67.7      & 40.8      & Kurmanji              & 78.3      & 55.2      \\ \hline
\multicolumn{1}{|l|}{Pashto}     & 55.2      & 34.3      & Zulu                  & 82.5      & 37.7      \\ \hline
\multicolumn{1}{|l|}{Turkish}    & 67.3      & 33.8      & Tokpisin              & 61.1      & 32.3      \\ \hline
\multicolumn{1}{|l|}{Tagalog}    & 54        & 35.8      & Cebuano               & 80.3      & 46.1      \\ \hline
\multicolumn{1}{|l|}{Vietnamese} & 86.3      & 54.8      & Kazakh                & 69.4      & 41        \\ \hline
\multicolumn{1}{|l|}{Haitian}    & 62.3      & 37.3      & Telugu                & 109.7     & 86        \\ \hline
\multicolumn{1}{|l|}{Swahili}    & 60.1      & 29.2      & Lithuanian            & 64.2      & 36.1      \\ \hline
\multicolumn{1}{|l|}{Lao}        & 83        & 46.8      & Guarani               & 67.5      & 40.9      \\ \hline
\end{tabular}
\caption{IARPA-BABEL monolingual training baseline}
\label{table:monoASR}
\end{table}
  \end{section}

  \begin{section}{切除研究 (Ablation Study)}
    \ref{progress} 中，在訓練策略上省略了許多能讓系統表現更好的技巧如以下：
    \begin{enumerate}[itemsep=-2mm]
      \item 利用該語言的文本(不含音訊)進行語言模型的預訓練，並加入解碼器的損失函數中，在 IARPA-BABEL 中，可以選擇用非 LLP 的翻譯作為該訓練文本。
      \item 資料增強：如在頻譜上做時間扭曲 (time warping)、遮蓋特定頻域的信號...等。
      \item 加上更好的特徵抽取層，如在編碼器前端使用 VGG 作為頻譜特徵抽取層。
      \item 增加更多訓練資料
    \end{enumerate}
    藉由加入以上方法，觀察元學習帶來的表現進步主要可能來自於更有效率地學習語言模型、或更有效率地學習聲學模型。
  \end{section}

  \begin{section}{泛化性分析}
    \begin{enumerate}[itemsep=-1mm]
      \item 置換訓練語言與目標語言，觀察元學習帶來的進步是否與語言相似程度無關。若有關，那是與語言間的何種關係有關(如地理位置、是否屬於相同語系)，而在 baseline model 是否也有類似觀察，可提供日後研究者在對目標語言有一定了解的前提下，如何選擇訓練語種的參考。
      \item 目前使用的架構為 LAS \cite{chan2016listen}，亦可測試於 DeepSpeech、Wav2letter...等架構，觀察元學習帶來的表現進步是否與模型無關。
      \item 設計課程學習 (Curriculum Learning) 的實驗，讓模型先完成簡單的任務再開始進行較困難的任務，觀察元學習所學習到的初始化參數是否能在簡單任務中不用訓練便能有好的預測結果，檢驗兩者之間是否有所關聯。
    \end{enumerate}   
  \end{section}
\end{section}

\bibliographystyle{plainnat}
\bibliography{M335}

\end{document}
