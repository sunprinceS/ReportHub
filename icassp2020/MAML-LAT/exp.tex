\section{Experiment}
\label{sec:exp}

%Lee:我覺得這章太多節了
%Hsu: 並沒有提到現在只有 greedy decode 而已

%\input{figs/llp_table.tex}
\label{ssec:exp-setup}
In this work, we used data from the IARPA BABEL project \cite{gales2014speech}. 
The corpus is mainly composed of conversational telephone speech (CTS). We selected 6 languages as non-target languages for multilingual pretraining: 
Bengali (Bn), Tagalog (Tl), Zulu (Zu), Turkish (Tr), Lithuanian (Lt), Guarani (Gn), and 4 target languages for adaptation: Vietnamese (Vi), Swahili (Sw), Tamil (Ta), Kurmanji (Ku), and experimented different combinations of non-target languages for pretraining.
Each language has Full Language Pack (FLP) and Limited Language Pack (LLP), which consists of 10\% of FLP.
%這裡解釋 FLP LLP

We followed the recipe provided by Espnet \cite{watanabe2018espnet} for data preprocessing and final score evaluation.  We used 80-dimensional Mel-filterbank and 3-dimensional pitch features as acoustic features. The size of the sliding window is 25ms, and the stride is 10ms. We used the shared encoder with a 6-layer VGG extractor with downsampling and a 6-layer bidirectional LSTM network with 360 cells in each direction used in the previous work \cite{dalmia2018sequence}.



\textbf{Meta Learning}. For each episode, we use a single gradient step of language-specific learning with SGD when computing the meta gradient. Noted that in Eq.~\ref{eq:meta-grad}, if we expand the loss term in the summation via Eq.~\ref{eq:fine-tune}, we will find the second-order derivative of $\theta$ appear. For computation efficiency, some previous works \cite{finn2017model, nichol2018reptile} showed that we could ignore the second-order term without affecting the performance too much.\\
Therefore, we approximate Eq.~\ref{eq:meta-grad} as follows.

\begin{equation}
  \theta \leftarrow \theta - \eta^\prime \sum_k \nabla_{\textcolor{red}{\theta_k^\prime}} \mathcal{L}_{D^{te}_k}(\theta^\prime_k, \theta^\prime_{h,k})
\end{equation}
Also known as First-order MAML (FOMAML).

%TODO: 說用 beam 還是 greedy ??
We multilingually pretrained the model for 100K steps for both MultiASR and MetaASR. When adapting to one certain language, we use the LLP of the other three languages as validation sets to decide which pretraining step of the pretrained model we should pick. Then we fine-tuned the monolingual model 18 epochs for the target language on its FLP, 20 epochs on its LLP, and evaluated the performance on the test set via greedy decoding.

