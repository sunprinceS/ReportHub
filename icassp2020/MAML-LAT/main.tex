% Template for ICASSP-2020 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
%\usepackage{multicol}
%\usepackage{multirow}
\documentclass{article}
\usepackage{color}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{spconf,amsmath,graphicx}
\usepackage{siunitx}
\usepackage{tikz}
\usepackage{pgfplotstable}
\usepackage{subfig}
\usepackage{float}
%\usepackage[preprint]{spconf}
\usepackage[OT1]{fontenc} % TODO: 之後放到 overleaf 要移掉！！

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}
\pgfplotsset{compat=1.16}

% ------
\title{Meta Learning for End-to-End Low-Resource Speech Recognition}
%
% Single address.
% ---------------
%\name{Jui-Yang Hsu, Yuan-Jui Chen,  Hung-yi Lee}
\name{Jui-Yang Hsu\qquad Yuan-Jui Chen\qquad  Hung-yi Lee}
\address{National Taiwan University \\
\small{\texttt{\{r07921053, r079xxxxx, hungyilee\}@ntu.edu.tw}}}
%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%
\begin{document}
%\ninept
%
\maketitle
%
\begin{abstract}
  %\textcolor{red}{(TBD BEGIN)} With the recent advances of deep learning, integrating the main modules of automatic speech recognition (ASR) such as acoustic model, pronunciation lexicon and language model into a single end-to-end model is highly attractive. Connectionist Temporal Classification (CTC) lends itself on such end-to-end approach by introducing an additional blank symbol and specifically-designed loss function optimizing to generate the correct character sequences from the speech signal directly, without framewise phoneme alignment in advance. With many recent results, end-to-end deep learning has created larger interest in speech community. \textcolor{red}{(TBD END)}
\end{abstract}
%
\begin{keywords}
  meta-learning, low-resource, multi-lingual speech recognition, language adaptation, IARPA-BABEL
\end{keywords}
%
\input{intro.tex}
\input{approach.tex}
\input{exp.tex}
\input{result.tex}

\section{Conclusion}
\label{sec:conclusion}
In this paper, we proposed the meta learning approach to multilingual pretraining for speech recognition. The initial experimental results showed its potential in pretraining. In future work, We plan to use more languages (from IARPA BABEL or other corpora) and different combinations for pretraining to evaluate the effectiveness of MetaASR more extensively. 

In addition, based on MAML's model-agnostic property, this approach can be applied to other network architecture like Seq2seq model, and even different applications other than speech recognition in the speech community.

% Below is an example of how to insert images. Delete the ``\vspace'' line,
% uncomment the preceding line ``\centerline...'' and replace ``imageX.ps''
% with a suitable PostScript file name.
% -------------------------------------------------------------------------


% To start a new column (but not a new page) and help balance the last-page
% column length use \vfill\pagebreak.
% -------------------------------------------------------------------------
%\vfill
%\pagebreak


%\section{REFERENCES}
% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\newpage
\bibliography{strings,refs}

% 以下不會放到 paper ，只是畫一些圖出來而已
%\newpage
\section{Appendix}

\begin{figure}[htb]
  \centering
  %\hspace{-2.2cm}
  \begin{tikzpicture}[trim axis left, trim axis right]

  \begin{axis}[
    width=\linewidth,
    height=6.5cm,
    legend entries={MultiASR, MetaASR} ,
    xlabel = {Number of pretraining steps ($\times 1000$)},
        xmin=5,
        %xmax=130,
        grid=both,
        legend style={at={(0.,.6)},anchor=south west},
        %legend pos=inner north west,
        ylabel={CER (\si{\percent}})]
  \addplot+[smooth]table{testing/mutli3-Tamil};
  \addplot+[smooth]table{testing/meta3-tamil};
   %\addplot[style=ultra thick,dashed,] coordinates {(0,0.557) (200,0.557)};
   %\addplot[style=ultra thick,dashed, gray] coordinates {(0,0.589) (200,0.589)};
   %\addplot[style=ultra thick,dashed, brown] coordinates {(0,0.628) (200,0.628)};
  \end{axis}
  \end{tikzpicture}
  %\caption{Pretrain on EN, FI, FR, NL, RM, RU, and evaluate on }
  \caption{The learning curves of CER on Tamil's LLP (near3)}
\end{figure}

\begin{figure}[htb]
  \centering
  %\hspace{-2.2cm}
  \begin{tikzpicture}[trim axis left, trim axis right]

  \begin{axis}[
    width=\linewidth,
    height=6.5cm,
    legend entries={MultiASR, MetaASR} ,
    xlabel = {Number of pretraining steps ($\times 1000$)},
        xmin=5,
        %xmax=130,
        grid=both,
        legend style={at={(0.,.7)},anchor=south west},
        %legend pos=inner north west,
        ylabel={CER (\si{\percent}})]
  \addplot+[smooth]table{testing/mutli6-tamil};
  \addplot+[smooth]table{testing/meta6-tamil};
   %\addplot[style=ultra thick,dashed,] coordinates {(0,0.557) (200,0.557)};
   %\addplot[style=ultra thick,dashed, gray] coordinates {(0,0.589) (200,0.589)};
   %\addplot[style=ultra thick,dashed, brown] coordinates {(0,0.628) (200,0.628)};
  \end{axis}
  \end{tikzpicture}
  %\caption{Pretrain on EN, FI, FR, NL, RM, RU, and evaluate on }
  \caption{The learning curves of CER on Tamil's LLP (near6)}
\end{figure}
\end{document}
